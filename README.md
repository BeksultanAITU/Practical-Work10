# Practical_Work10

Практическая работа №10 посвящена измерению и анализу производительности параллельных программ на разных уровнях. Во всех заданиях использовались большие массивы данных, а результаты фиксировались по времени выполнения, чтобы можно было сравнить подходы и сделать понятные выводы.

---

## Таск 1. Анализ производительности CPU-параллельной программы (OpenMP)

В первом задании была реализована обработка большого массива на CPU с использованием OpenMP. На стороне программы создаётся массив случайных чисел размера `N` (например, 10 000 000 элементов), после чего вычисляются сумма, среднее значение и дисперсия. Для корректного сравнения сначала выполняется последовательная версия, которая используется как базовая линия. Затем запускается параллельная версия с разным числом потоков, а время измеряется через `omp_get_wtime()` только для вычислительной части, без учёта генерации данных.

После каждого запуска рассчитывается ускорение как отношение времени последовательной версии к времени параллельной. Далее для каждого числа потоков оценивается доля параллельной части программы через закон Амдала. Это позволяет показать, что даже при большом количестве потоков ускорение упирается в последовательную часть и накладные расходы, поэтому рост производительности ограничен.

Результаты выполнения программы представлены на скриншоте:

<p align="center">
  <img src="files/т1.jpg" width="520">
</p>

<p align="center">
  <img src="files/task1_flowchart.jpg" width="420">
</p>

После выполнения задания видно, что увеличение числа потоков улучшает скорость только до определённого момента. Дальше эффект снижается из-за последовательных участков и накладных расходов, что на практике подтверждает закон Амдала.

---

## Таск 2. Оптимизация доступа к памяти на GPU (CUDA)

Во втором задании была реализована CUDA-обработка массива и сравнение разных паттернов доступа к глобальной памяти. Первая версия ядра использует коалесцированный доступ, где соседние потоки обрабатывают соседние элементы. Вторая версия специально создаёт некоалесцированный доступ, когда каждый поток обращается к разрозненному индексу. Оба варианта измеряются с помощью `cudaEvent`, что позволяет получить время выполнения именно GPU-части.

Далее выполняется оптимизация за счёт использования разделяемой памяти. В оптимизированной версии блок потоков сначала загружает данные плиткой в shared memory, после чего выполняет вычисления и записывает результат обратно. Такой подход уменьшает влияние задержек глобальной памяти и делает обращение к данным более организованным.

Результаты выполнения программы представлены на скриншоте:

<p align="center">
  <img src="files/т2.jpg" width="520">
</p>

<p align="center">
  <img src="files/task2_flowchart.jpg" width="420">
</p>

После выполнения задания видно, что коалесцированный доступ заметно быстрее, чем некоалесцированный, потому что GPU может объединять обращения к памяти в крупные транзакции. Использование shared memory дополнительно улучшает ситуацию, так как часть операций выполняется на более быстрой памяти блока.

---

## Таск 3. Профилирование гибридного приложения CPU + GPU

В третьем задании была реализована гибридная программа, где первая часть массива обрабатывается на CPU, а вторая часть обрабатывается на GPU. Основная цель заключается в том, чтобы показать накладные расходы передачи данных и влияние асинхронности. Для этого используется `cudaMemcpyAsync` и CUDA streams, что позволяет перекрывать копирование и вычисления.

В программе сравниваются два подхода. В базовом варианте данные копируются на GPU синхронно и последовательно, затем запускается ядро и затем результат копируется обратно. В оптимизированном варианте используется pinned memory и асинхронные копирования в stream. Пока GPU копирует вторую половину массива, CPU параллельно обрабатывает первую половину. Затем выполняется вычисление на GPU и асинхронное копирование результата обратно.

Результаты выполнения программы представлены на скриншоте:

<p align="center">
  <img src="files/т3.jpg" width="520">
</p>

<p align="center">
  <img src="files/task3_flowchart.jpg" width="420">
</p>

После выполнения задания видно, что даже если вычисления на GPU быстрые, итоговое время может сильно зависеть от передачи данных. Асинхронная передача и pinned memory уменьшают накладные расходы и позволяют перекрывать CPU-работу с GPU-передачами, что делает гибридный подход более эффективным.

---

## Таск 4. Анализ масштабируемости распределённой программы (MPI)

В четвёртом задании была реализована MPI-программа для вычисления агрегатных функций над большим массивом. На процессе с `rank = 0` создаётся общий массив, затем данные распределяются между процессами через `MPI_Scatterv`, чтобы корректно учитывать случаи, когда размер массива не делится нацело. Каждый процесс вычисляет локальную сумму, минимум и максимум, после чего результаты собираются на root через `MPI_Reduce`. Дополнительно используется `MPI_Allreduce`, чтобы сравнить стоимость операций, когда итог нужен всем процессам.

Для анализа масштабируемости рассматриваются два режима. В strong scaling общий размер задачи фиксирован, и измеряется, как меняется время при увеличении числа процессов. В weak scaling размер задачи растёт пропорционально числу процессов, чтобы нагрузка на процесс оставалась примерно постоянной. В обоих режимах измерение выполняется через `MPI_Wtime()`.

Результаты выполнения программы представлены на скриншоте:

<p align="center">
  <img src="files/т4.jpg" width="520">
</p>

<p align="center">
  <img src="files/task4_flowchart.jpg" width="420">
</p>
После выполнения задания видно, что strong scaling работает хорошо только до тех пор, пока коммуникации не начинают доминировать. При большом числе процессов вычислительная часть уменьшается, но стоимость `MPI_Reduce` и особенно `MPI_Allreduce` становится заметной. В weak scaling время обычно растёт медленнее, но тоже ограничивается коммуникациями и пропускной способностью сети.

---

# Контрольные вопросы

## 1. В чём отличие измерения времени выполнения от профилирования?

Измерение времени показывает итоговую длительность работы выбранного участка программы и даёт общий ответ “сколько заняло”. Профилирование показывает, почему заняло именно столько. Оно разбивает время по функциям, ядрам, копированиям, синхронизациям и позволяет увидеть узкие места, например ожидания памяти, простои потоков, перегруженные ядра или затраты на коммуникации.

## 2. Какие виды узких мест характерны для CPU, GPU и распределённых программ?

Для CPU типичные узкие места связаны с ограничениями по памяти, кэшам и конкуренцией потоков, а также с последовательными участками алгоритма. Для GPU ключевыми узкими местами являются глобальная память, коалесцирование доступа, регистры, shared memory и расхождение ветвлений внутри варпов. Для MPI-программ главный источник проблем — коммуникации, синхронизация и задержки сети, а также дисбаланс нагрузки между процессами.

## 3. Почему увеличение числа потоков или процессов не всегда приводит к ускорению?

Потому что часть программы остаётся последовательной и не ускоряется. Дополнительно растут накладные расходы: создание и управление потоками, синхронизация, обмен данными и конкуренция за общие ресурсы. На GPU это может быть ограничение пропускной способности памяти, а в MPI — задержки и стоимость коллективных операций.

## 4. Как законы Амдала и Густафсона применяются при анализе масштабируемости?

Закон Амдала оценивает ускорение при фиксированном размере задачи и показывает, что последовательная часть ограничивает максимальный выигрыш. Закон Густафсона рассматривает ситуацию, когда размер задачи растёт вместе с числом процессоров, и тогда масштабируемость может быть выше, потому что доля параллельной работы увеличивается относительно последовательной.

## 5. Какие факторы наиболее критичны для производительности гибридных приложений?

Самые критичные факторы — это стоимость передачи данных CPU↔GPU, возможность перекрытия копирований и вычислений через streams, использование pinned memory, организация данных и минимизация синхронизаций. На практике гибридный подход выигрывает тогда, когда вычисления достаточно тяжёлые, а обмен данными организован так, чтобы не превращаться в главный узкий участок.

---

# Заключение

В ходе практической работы было показано, что производительность параллельных программ определяется не только количеством потоков или процессов, но и реальными ограничениями архитектуры. На CPU ускорение ограничивается последовательной частью и накладными расходами, что хорошо описывается законом Амдала. На GPU ключевую роль играет работа с памятью: коалесцированный доступ и использование shared memory дают заметный выигрыш даже без изменения математической части. В гибридных приложениях решающим фактором становится передача данных между CPU и GPU, и оптимизация через pinned memory и асинхронные копирования помогает уменьшить накладные расходы. В распределённых MPI-программах масштабируемость ограничивается коммуникациями, поэтому при росте числа процессов коллективные операции начинают доминировать над локальными вычислениями. Все задания в сумме демонстрируют, что эффективная оптимизация начинается с измерений, а затем требует точечной работы с узкими местами каждой конкретной платформы.
